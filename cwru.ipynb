{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cwru.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/fboldt/algoritmos/blob/master/cwru.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Con6WhcSFxxm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CWRU files location"
      ]
    },
    {
      "metadata": {
        "id": "uSt7Dc1-nzQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "aquisitions = {}\n",
        "# Normal\n",
        "aquisitions[\"Normal_0\"] = \"97.mat\"\n",
        "aquisitions[\"Normal_1\"] = \"98.mat\"\n",
        "aquisitions[\"Normal_2\"] = \"99.mat\"\n",
        "aquisitions[\"Normal_3\"] = \"100.mat\"\n",
        "# DE Inner Race 0.007 inches\n",
        "aquisitions[\"DEIR007_0\"] = \"105.mat\"\n",
        "aquisitions[\"DEIR007_1\"] = \"106.mat\"\n",
        "aquisitions[\"DEIR007_2\"] = \"107.mat\"\n",
        "aquisitions[\"DEIR007_3\"] = \"108.mat\"\n",
        "# DE Inner Race 0.014 inches\n",
        "aquisitions[\"DEIR014_0\"] = \"169.mat\"\n",
        "aquisitions[\"DEIR014_1\"] = \"170.mat\"\n",
        "aquisitions[\"DEIR014_2\"] = \"171.mat\"\n",
        "aquisitions[\"DEIR014_3\"] = \"172.mat\"\n",
        "# DE Inner Race 0.021 inches\n",
        "aquisitions[\"DEIR021_0\"] = \"209.mat\"\n",
        "aquisitions[\"DEIR021_1\"] = \"210.mat\"\n",
        "aquisitions[\"DEIR021_2\"] = \"211.mat\"\n",
        "aquisitions[\"DEIR021_3\"] = \"212.mat\"\n",
        "# DE Ball 0.007 inches\n",
        "aquisitions[\"DEB007_0\"] = \"118.mat\"\n",
        "aquisitions[\"DEB007_1\"] = \"119.mat\"\n",
        "aquisitions[\"DEB007_2\"] = \"120.mat\"\n",
        "aquisitions[\"DEB007_3\"] = \"121.mat\"\n",
        "# DE Ball 0.014 inches\n",
        "aquisitions[\"DEB014_0\"] = \"185.mat\"\n",
        "aquisitions[\"DEB014_1\"] = \"186.mat\"\n",
        "aquisitions[\"DEB014_2\"] = \"187.mat\"\n",
        "aquisitions[\"DEB014_3\"] = \"188.mat\"\n",
        "# DE Ball 0.021 inches\n",
        "aquisitions[\"DEB021_0\"] = \"222.mat\"\n",
        "aquisitions[\"DEB021_1\"] = \"223.mat\"\n",
        "aquisitions[\"DEB021_2\"] = \"224.mat\"\n",
        "aquisitions[\"DEB021_3\"] = \"225.mat\"\n",
        "# DE Outer race 0.007 inches centered @6:00\n",
        "aquisitions[\"DEOR007@6_0\"] = \"130.mat\"\n",
        "aquisitions[\"DEOR007@6_1\"] = \"131.mat\"\n",
        "aquisitions[\"DEOR007@6_2\"] = \"132.mat\"\n",
        "aquisitions[\"DEOR007@6_3\"] = \"133.mat\"\n",
        "# DE Outer race 0.014 inches centered @6:00\n",
        "aquisitions[\"DEOR014@6_0\"] = \"197.mat\"\n",
        "aquisitions[\"DEOR014@6_1\"] = \"198.mat\"\n",
        "aquisitions[\"DEOR014@6_2\"] = \"199.mat\"\n",
        "aquisitions[\"DEOR014@6_3\"] = \"200.mat\"\n",
        "# DE Outer race 0.021 inches centered @6:00\n",
        "aquisitions[\"DEOR021@6_0\"] = \"234.mat\"\n",
        "aquisitions[\"DEOR021@6_1\"] = \"235.mat\"\n",
        "aquisitions[\"DEOR021@6_2\"] = \"236.mat\"\n",
        "aquisitions[\"DEOR021@6_3\"] = \"237.mat\"\n",
        "# DE Outer race 0.007 inches centered @3:00\n",
        "aquisitions[\"DEOR007@3_0\"] = \"144.mat\"\n",
        "aquisitions[\"DEOR007@3_1\"] = \"145.mat\"\n",
        "aquisitions[\"DEOR007@3_2\"] = \"146.mat\"\n",
        "aquisitions[\"DEOR007@3_3\"] = \"147.mat\"\n",
        "# DE Outer race 0.021 inches centered @3:00\n",
        "aquisitions[\"DEOR021@3_0\"] = \"246.mat\"\n",
        "aquisitions[\"DEOR021@3_1\"] = \"247.mat\"\n",
        "aquisitions[\"DEOR021@3_2\"] = \"248.mat\"\n",
        "aquisitions[\"DEOR021@3_3\"] = \"249.mat\"\n",
        "# DE Outer race 0.007 inches centered @12:00\n",
        "aquisitions[\"DEOR007@12_0\"] = \"156.mat\"\n",
        "aquisitions[\"DEOR007@12_1\"] = \"158.mat\"\n",
        "aquisitions[\"DEOR007@12_2\"] = \"159.mat\"\n",
        "aquisitions[\"DEOR007@12_3\"] = \"160.mat\"\n",
        "# DE Outer race 0.021 inches centered @12:00\n",
        "aquisitions[\"DEOR021@12_0\"] = \"258.mat\"\n",
        "aquisitions[\"DEOR021@12_1\"] = \"259.mat\"\n",
        "aquisitions[\"DEOR021@12_2\"] = \"260.mat\"\n",
        "aquisitions[\"DEOR021@12_3\"] = \"261.mat\"\n",
        "# FE Inner Race 0.007 inches\n",
        "aquisitions[\"FEIR007_0\"] = \"278.mat\"\n",
        "aquisitions[\"FEIR007_1\"] = \"279.mat\"\n",
        "aquisitions[\"FEIR007_2\"] = \"280.mat\"\n",
        "aquisitions[\"FEIR007_3\"] = \"281.mat\"\n",
        "# FE Inner Race 0.014 inches\n",
        "aquisitions[\"FEIR014_0\"] = \"274.mat\"\n",
        "aquisitions[\"FEIR014_1\"] = \"275.mat\"\n",
        "aquisitions[\"FEIR014_2\"] = \"276.mat\"\n",
        "aquisitions[\"FEIR014_3\"] = \"277.mat\"\n",
        "# FE Inner Race 0.021 inches\n",
        "aquisitions[\"FEIR021_0\"] = \"270.mat\"\n",
        "aquisitions[\"FEIR021_1\"] = \"271.mat\"\n",
        "aquisitions[\"FEIR021_2\"] = \"272.mat\"\n",
        "aquisitions[\"FEIR021_3\"] = \"273.mat\"\n",
        "# FE Ball 0.007 inches\n",
        "aquisitions[\"FEB007_0\"] = \"282.mat\"\n",
        "aquisitions[\"FEB007_1\"] = \"283.mat\"\n",
        "aquisitions[\"FEB007_2\"] = \"284.mat\"\n",
        "aquisitions[\"FEB007_3\"] = \"285.mat\"\n",
        "# FE Ball 0.014 inches\n",
        "aquisitions[\"FEB014_0\"] = \"286.mat\"\n",
        "aquisitions[\"FEB014_1\"] = \"287.mat\"\n",
        "aquisitions[\"FEB014_2\"] = \"288.mat\"\n",
        "aquisitions[\"FEB014_3\"] = \"289.mat\"\n",
        "# FE Ball 0.021 inches\n",
        "aquisitions[\"FEB021_0\"] = \"290.mat\"\n",
        "aquisitions[\"FEB021_1\"] = \"291.mat\"\n",
        "aquisitions[\"FEB021_2\"] = \"292.mat\"\n",
        "aquisitions[\"FEB021_3\"] = \"293.mat\"\n",
        "# FE Outer race 0.007 inches centered @6:00\n",
        "aquisitions[\"FEOR007@6_0\"] = \"294.mat\"\n",
        "aquisitions[\"FEOR007@6_1\"] = \"295.mat\"\n",
        "aquisitions[\"FEOR007@6_2\"] = \"296.mat\"\n",
        "aquisitions[\"FEOR007@6_3\"] = \"297.mat\"\n",
        "# FE Outer race 0.007 inches centered @3:00\n",
        "aquisitions[\"FEOR007@3_0\"] = \"298.mat\"\n",
        "aquisitions[\"FEOR007@3_1\"] = \"299.mat\"\n",
        "aquisitions[\"FEOR007@3_2\"] = \"300.mat\"\n",
        "aquisitions[\"FEOR007@3_3\"] = \"301.mat\"\n",
        "# FE Outer race 0.014 inches centered @3:00\n",
        "aquisitions[\"FEOR014@3_0\"] = \"310.mat\"\n",
        "aquisitions[\"FEOR014@3_1\"] = \"309.mat\"\n",
        "aquisitions[\"FEOR014@3_2\"] = \"311.mat\"\n",
        "aquisitions[\"FEOR014@3_3\"] = \"312.mat\"\n",
        "# FE Outer race 0.007 inches centered @12:00\n",
        "aquisitions[\"FEOR007@12_0\"] = \"302.mat\"\n",
        "aquisitions[\"FEOR007@12_1\"] = \"305.mat\"\n",
        "aquisitions[\"FEOR007@12_2\"] = \"306.mat\"\n",
        "aquisitions[\"FEOR007@12_3\"] = \"307.mat\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_4WiLNSyrMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generate a dictionary linking the labels with values to keep consistence"
      ]
    },
    {
      "metadata": {
        "id": "zHDI4LWbyoP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labels_dict(aquisitions):\n",
        "  labels_dict = {}\n",
        "  value = 0\n",
        "  for key in aquisitions.keys():\n",
        "    label = key.split('_')[0]\n",
        "    if not label in labels_dict:\n",
        "      labels_dict[label] = value\n",
        "      value += 1\n",
        "  return labels_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Rz0cKigF7q6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convert Matlab file into tensors"
      ]
    },
    {
      "metadata": {
        "id": "2tn5IFqkCHQx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "def aquisition2tensor(file_name, sample_size=512):\n",
        "  print(file_name, end=' ')\n",
        "  matlab_file = scipy.io.loadmat(file_name)\n",
        "  DE_time = [key for key in matlab_file if key.endswith(\"DE_time\")][0] #Find the DRIVE END aquisition key name\n",
        "  FE_time = [key for key in matlab_file if key.endswith(\"FE_time\")][0] #Find the FAN END aquisition key name\n",
        "  signal_begin = 0\n",
        "  aquisition_size = max(len(matlab_file[DE_time]),len(matlab_file[FE_time]))\n",
        "  DE_samples = []\n",
        "  FE_samples = []\n",
        "  while signal_begin + sample_size < aquisition_size:\n",
        "    DE_samples.append([item for sublist in matlab_file[DE_time][signal_begin:signal_begin+sample_size] for item in sublist])\n",
        "    FE_samples.append([item for sublist in matlab_file[FE_time][signal_begin:signal_begin+sample_size] for item in sublist])\n",
        "    signal_begin += sample_size\n",
        "  sample_tensor = np.stack([DE_samples,FE_samples],axis=2).astype('float32')\n",
        "  return sample_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-G4gjyuvA7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extract datasets from aquisitions"
      ]
    },
    {
      "metadata": {
        "id": "YC4rhGwWMKT7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concatenate_datasets(xd,yd,xo,yo):\n",
        "  if xd is None or yd is None:\n",
        "    xd = xo\n",
        "    yd = yo\n",
        "  else:\n",
        "    xd = np.concatenate((xd,xo))\n",
        "    yd = np.concatenate((yd,yo))\n",
        "  return xd,yd\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "def aquisitions_from_load(load, aquisitions, labels_dict,\n",
        "                          url=\"http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/\"\n",
        "                         ):\n",
        "  samples = None\n",
        "  labels = None\n",
        "  for key in aquisitions:\n",
        "    if key.endswith(\"_\"+str(load)):\n",
        "      file_name = aquisitions[key]\n",
        "      urllib.request.urlretrieve(url+file_name, file_name)\n",
        "      aquisition_samples = aquisition2tensor(file_name)\n",
        "      aquisition_labels = np.ones(aquisition_samples.shape[0])*labels_dict[key.split('_')[0]]\n",
        "      samples,labels = concatenate_datasets(samples,labels,aquisition_samples,aquisition_labels)\n",
        "  print(load)\n",
        "  return samples,labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfwmQa0RJMvb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exrtract samples"
      ]
    },
    {
      "metadata": {
        "id": "HSASqQsHcgtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "98e1a809-db61-4468-f83a-cd1b67f507b3"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "labels_dict = get_labels_dict(aquisitions)\n",
        "print(labels_dict.keys())\n",
        "\n",
        "x0,y0 = aquisitions_from_load(0,aquisitions,labels_dict)\n",
        "x1,y1 = aquisitions_from_load(1,aquisitions,labels_dict)\n",
        "x2,y2 = aquisitions_from_load(2,aquisitions,labels_dict)\n",
        "x3,y3 = aquisitions_from_load(3,aquisitions,labels_dict)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Normal', 'DEIR007', 'DEIR014', 'DEIR021', 'DEB007', 'DEB014', 'DEB021', 'DEOR007@6', 'DEOR014@6', 'DEOR021@6', 'DEOR007@3', 'DEOR021@3', 'DEOR007@12', 'DEOR021@12', 'FEIR007', 'FEIR014', 'FEIR021', 'FEB007', 'FEB014', 'FEB021', 'FEOR007@6', 'FEOR007@3', 'FEOR014@3', 'FEOR007@12'])\n",
            "97.mat 105.mat 169.mat 209.mat 118.mat 185.mat 222.mat 130.mat 197.mat 234.mat 144.mat 246.mat 156.mat 258.mat 278.mat 274.mat 270.mat 282.mat 286.mat 290.mat 294.mat 298.mat 310.mat 302.mat 0\n",
            "98.mat 106.mat 170.mat 210.mat 119.mat 186.mat 223.mat 131.mat 198.mat 235.mat 145.mat 247.mat 158.mat 259.mat 279.mat 275.mat 271.mat 283.mat 287.mat 291.mat 295.mat 299.mat 309.mat 305.mat 1\n",
            "99.mat 107.mat 171.mat 211.mat 120.mat 187.mat 224.mat 132.mat 199.mat 236.mat 146.mat 248.mat 159.mat 260.mat 280.mat 276.mat 272.mat 284.mat 288.mat 292.mat 296.mat 300.mat 311.mat 306.mat 2\n",
            "100.mat 108.mat 172.mat 212.mat 121.mat 188.mat 225.mat 133.mat 200.mat 237.mat 147.mat 249.mat 160.mat 261.mat 281.mat 277.mat 273.mat 285.mat 289.mat 293.mat 297.mat 301.mat 312.mat 307.mat 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QgaHQFpBuDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define f1-score macro averaged"
      ]
    },
    {
      "metadata": {
        "id": "HdRZ_4pJBzfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def f1_score_macro(y_true,y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPLjBiao2GIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define architecture model"
      ]
    },
    {
      "metadata": {
        "id": "fpcZrU4Mu9Vk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "23a91f25-290a-4712-db7b-9a7ee264a603"
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "signal_input = Input(shape=(x0.shape[1],x0.shape[-1]), dtype='float32', name='signal')\n",
        "x = layers.Conv1D(64, 128, activation='relu')(signal_input)\n",
        "x = layers.Flatten()(x)\n",
        "condition_output = layers.Dense(len(labels_dict),activation='softmax',name='condition')(x)\n",
        "\n",
        "model = Model(signal_input, condition_output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['accuracy',f1_score_macro])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "signal (InputLayer)          (None, 512, 2)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 385, 64)           16448     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 24640)             0         \n",
            "_________________________________________________________________\n",
            "condition (Dense)            (None, 24)                591384    \n",
            "=================================================================\n",
            "Total params: 607,832\n",
            "Trainable params: 607,832\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FfW63j1-6VgZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "history = model.fit(x_train,y_train,epochs=10,\n",
        "                    validation_split=0.33)#validation_data=(x_val,y_val))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1,len(loss_values)+1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "epochs = range(1,len(acc_values)+1)\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "results\n",
        "'''"
      ]
    },
    {
      "metadata": {
        "id": "y0xe9oYrW1lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Perform experiments"
      ]
    },
    {
      "metadata": {
        "id": "UU0B-KF-W0eV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1533
        },
        "outputId": "64f3e87e-c8ca-4f9c-bab4-b2fd4271dc0b"
      },
      "cell_type": "code",
      "source": [
        "loads = list(range(4))\n",
        "nrounds = 5\n",
        "results = []\n",
        "for i,fold in enumerate(loads):\n",
        "  print(fold)\n",
        "  x_test, y_test = eval('x'+str(fold)),eval('y'+str(fold))\n",
        "  y_test = to_categorical(y_test)\n",
        "  x_train,y_train = None,None\n",
        "  for tfold in loads[:i]+loads[i+1:]:\n",
        "    x_train,y_train = concatenate_datasets(x_train,y_train,eval('x'+str(tfold)),eval('y'+str(tfold)))\n",
        "  y_train = to_categorical(y_train)\n",
        "  print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
        "  for round in range(nrounds):\n",
        "    print(round+1)\n",
        "    history = model.fit(x_train,y_train,epochs=10,validation_split=0.33)\n",
        "    results.append(model.evaluate(x_test, y_test))\n",
        "    print(results[-1])\n",
        "  print(np.asarray(results[-nrounds:]))\n",
        "  print(np.mean(results[-nrounds:],axis=0),np.std(results[-nrounds:],axis=0))\n",
        "\n",
        "print(np.asarray(results))\n",
        "print(np.mean(results,axis=0),np.std(results,axis=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(19208, 512, 2) (19208, 24) (5931, 512, 2) (5931, 24)\n",
            "1\n",
            "Train on 12869 samples, validate on 6339 samples\n",
            "Epoch 1/10\n",
            "12869/12869 [==============================] - 7s 530us/step - loss: 0.0108 - acc: 0.8070 - f1_score_macro: 0.7912 - val_loss: 0.0028 - val_acc: 0.9598 - val_f1_score_macro: 0.9548\n",
            "Epoch 2/10\n",
            "12869/12869 [==============================] - 5s 354us/step - loss: 9.8281e-04 - acc: 0.9861 - f1_score_macro: 0.9869 - val_loss: 0.0013 - val_acc: 0.9820 - val_f1_score_macro: 0.9809\n",
            "Epoch 3/10\n",
            "12869/12869 [==============================] - 4s 343us/step - loss: 1.5371e-04 - acc: 0.9981 - f1_score_macro: 0.9979 - val_loss: 0.0030 - val_acc: 0.9478 - val_f1_score_macro: 0.9469\n",
            "Epoch 4/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 5.8523e-05 - acc: 0.9992 - f1_score_macro: 0.9993 - val_loss: 6.8767e-04 - val_acc: 0.9891 - val_f1_score_macro: 0.9892\n",
            "Epoch 5/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 4.7393e-05 - acc: 0.9993 - f1_score_macro: 0.9993 - val_loss: 0.0010 - val_acc: 0.9839 - val_f1_score_macro: 0.9832\n",
            "Epoch 6/10\n",
            "12869/12869 [==============================] - 4s 336us/step - loss: 4.0025e-05 - acc: 0.9994 - f1_score_macro: 0.9994 - val_loss: 6.4551e-04 - val_acc: 0.9907 - val_f1_score_macro: 0.9903\n",
            "Epoch 7/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 3.5096e-05 - acc: 0.9995 - f1_score_macro: 0.9995 - val_loss: 5.5102e-04 - val_acc: 0.9913 - val_f1_score_macro: 0.9910\n",
            "Epoch 8/10\n",
            "12869/12869 [==============================] - 4s 331us/step - loss: 3.2961e-05 - acc: 0.9995 - f1_score_macro: 0.9995 - val_loss: 0.0011 - val_acc: 0.9806 - val_f1_score_macro: 0.9809\n",
            "Epoch 9/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 3.2876e-05 - acc: 0.9995 - f1_score_macro: 0.9996 - val_loss: 0.0026 - val_acc: 0.9550 - val_f1_score_macro: 0.9538\n",
            "Epoch 10/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 3.8442e-05 - acc: 0.9995 - f1_score_macro: 0.9995 - val_loss: 5.5502e-04 - val_acc: 0.9924 - val_f1_score_macro: 0.9919\n",
            "5931/5931 [==============================] - 1s 157us/step\n",
            "[0.00366861250292258, 0.944697352941835, 0.9445013559562041]\n",
            "2\n",
            "Train on 12869 samples, validate on 6339 samples\n",
            "Epoch 1/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 3.6858e-05 - acc: 0.9995 - f1_score_macro: 0.9995 - val_loss: 6.2917e-04 - val_acc: 0.9910 - val_f1_score_macro: 0.9904\n",
            "Epoch 2/10\n",
            "12869/12869 [==============================] - 4s 331us/step - loss: 3.4014e-05 - acc: 0.9995 - f1_score_macro: 0.9995 - val_loss: 4.8218e-04 - val_acc: 0.9923 - val_f1_score_macro: 0.9926\n",
            "Epoch 3/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 3.3718e-05 - acc: 0.9995 - f1_score_macro: 0.9996 - val_loss: 5.1472e-04 - val_acc: 0.9927 - val_f1_score_macro: 0.9927\n",
            "Epoch 4/10\n",
            "12869/12869 [==============================] - 4s 330us/step - loss: 3.1629e-05 - acc: 0.9995 - f1_score_macro: 0.9996 - val_loss: 8.8803e-04 - val_acc: 0.9866 - val_f1_score_macro: 0.9859\n",
            "Epoch 5/10\n",
            "12869/12869 [==============================] - 4s 331us/step - loss: 2.4022e-05 - acc: 0.9996 - f1_score_macro: 0.9996 - val_loss: 5.8517e-04 - val_acc: 0.9910 - val_f1_score_macro: 0.9911\n",
            "Epoch 6/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 1.3620e-05 - acc: 0.9998 - f1_score_macro: 0.9998 - val_loss: 8.7074e-04 - val_acc: 0.9847 - val_f1_score_macro: 0.9850\n",
            "Epoch 7/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 3.3105e-06 - acc: 0.9999 - f1_score_macro: 0.9999 - val_loss: 4.3902e-04 - val_acc: 0.9931 - val_f1_score_macro: 0.9931\n",
            "Epoch 8/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 6.8056e-08 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 4.0149e-04 - val_acc: 0.9938 - val_f1_score_macro: 0.9937\n",
            "Epoch 9/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 1.4423e-09 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 4.5014e-04 - val_acc: 0.9924 - val_f1_score_macro: 0.9923\n",
            "Epoch 10/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 1.0519e-09 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 4.0756e-04 - val_acc: 0.9931 - val_f1_score_macro: 0.9932\n",
            "5931/5931 [==============================] - 1s 159us/step\n",
            "[0.003968365721204281, 0.9379531276847114, 0.9370804961184751]\n",
            "3\n",
            "Train on 12869 samples, validate on 6339 samples\n",
            "Epoch 1/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 8.4647e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.6523e-04 - val_acc: 0.9937 - val_f1_score_macro: 0.9937\n",
            "Epoch 2/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 7.4358e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.8986e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9936\n",
            "Epoch 3/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 6.6448e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.9751e-04 - val_acc: 0.9932 - val_f1_score_macro: 0.9933\n",
            "Epoch 4/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 6.1053e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.8228e-04 - val_acc: 0.9935 - val_f1_score_macro: 0.9936\n",
            "Epoch 5/10\n",
            "12869/12869 [==============================] - 4s 330us/step - loss: 5.6600e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.7066e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9935\n",
            "Epoch 6/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 5.2587e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.4813e-04 - val_acc: 0.9937 - val_f1_score_macro: 0.9940\n",
            "Epoch 7/10\n",
            "12869/12869 [==============================] - 4s 333us/step - loss: 4.9745e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.6449e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9935\n",
            "Epoch 8/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 4.7167e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.8939e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9934\n",
            "Epoch 9/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 4.4920e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.4991e-04 - val_acc: 0.9937 - val_f1_score_macro: 0.9939\n",
            "Epoch 10/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 4.2780e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.5914e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9935\n",
            "5931/5931 [==============================] - 1s 161us/step\n",
            "[0.0037848686968616056, 0.9418310572075574, 0.9418438602269671]\n",
            "4\n",
            "Train on 12869 samples, validate on 6339 samples\n",
            "Epoch 1/10\n",
            "12869/12869 [==============================] - 4s 331us/step - loss: 4.0926e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.6012e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9937\n",
            "Epoch 2/10\n",
            "12869/12869 [==============================] - 4s 330us/step - loss: 3.9161e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.4599e-04 - val_acc: 0.9937 - val_f1_score_macro: 0.9940\n",
            "Epoch 3/10\n",
            "12869/12869 [==============================] - 4s 330us/step - loss: 3.7715e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.7195e-04 - val_acc: 0.9935 - val_f1_score_macro: 0.9936\n",
            "Epoch 4/10\n",
            "12869/12869 [==============================] - 4s 332us/step - loss: 3.6435e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.1464e-04 - val_acc: 0.9946 - val_f1_score_macro: 0.9948\n",
            "Epoch 5/10\n",
            "12869/12869 [==============================] - 4s 331us/step - loss: 3.5705e-10 - acc: 1.0000 - f1_score_macro: 1.0000 - val_loss: 3.7181e-04 - val_acc: 0.9934 - val_f1_score_macro: 0.9936\n",
            "Epoch 6/10\n",
            " 9088/12869 [====================>.........] - ETA: 0s - loss: 3.2820e-10 - acc: 1.0000 - f1_score_macro: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tv7O1HexF8Ct",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "np.savetxt(\"results.csv\", np.asarray(results), delimiter=\", \", newline=\"\\r\\n\")\n",
        "files.download('results.csv') \n",
        "np.savetxt(\"summary.csv\", np.asarray(np.asarray([np.mean(results,axis=0),np.std(results,axis=0)])), delimiter=\", \", newline=\"\\r\\n\")\n",
        "files.download('summary.csv') \n",
        "with open('model.txt','w') as fh:\n",
        "    model.summary(print_fn=lambda x: fh.write(x + '\\r\\n'))\n",
        "files.download('model.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}